{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ML Project - Health Insurance Cost Estimation Project\n",
    "\n",
    "The project is divided into the following sections: \n",
    "- Data understanding and exploration\n",
    "- Data cleaning\n",
    "- Data preparation\n",
    "- Model building and evaluation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1. Data Understanding and Exploration\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#importing the required libraries\n",
    "import numpy as np \n",
    "import pandas as pd\n",
    "\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.style\n",
    "plt.style.use('classic')\n",
    "sns.set(rc={'figure.figsize':(30,10)})\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Reading the dataset\n",
    "df = pd.read_csv('Data.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Looking at the first few rows\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Summary of the dataset : 1460 rows, 81 columns\n",
    "df.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# checking for null values\n",
    "df_null_count = df.isna().sum().reset_index()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_null_count.columns = ['Variable','Null_Count']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_null_count.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2. Data cleaning\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Treating Null Values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# checking for percentage of null values\n",
    "df_null_count['Null_Percentage'] = round((df_null_count['Null_Count']/df.shape[0])*100,2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#  displaying null values greater than 75\n",
    "df_null_count[df_null_count['Null_Percentage']>=75]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#  displaying null values greater than 20 and less than 50\n",
    "df_null_count[(df_null_count['Null_Percentage']>=20) & (df_null_count['Null_Percentage']<50)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#  displaying null values less than 20\n",
    "df_null_count[(df_null_count['Null_Percentage']>0) & (df_null_count['Null_Percentage']<20)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Dropping few variables by looking into their dtype and their fill rate - "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# storing these values\n",
    "null_75 = df_null_count[df_null_count['Null_Percentage']>=75]\n",
    "null_50_75 = df_null_count[(df_null_count['Null_Percentage']>=50) & (df_null_count['Null_Percentage']<75)]\n",
    "null_20_50 = df_null_count[(df_null_count['Null_Percentage']>=20) & (df_null_count['Null_Percentage']<50)]\n",
    "null_0_20 = df_null_count[(df_null_count['Null_Percentage']>0) & (df_null_count['Null_Percentage']<20)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df[null_75['Variable']].info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df[null_50_75['Variable']].info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df[null_20_50['Variable']].info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df[null_0_20['Variable']].info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df['Year_last_admitted'].unique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df['Year_last_admitted'].fillna(0,inplace = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df['bmi'].unique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df['bmi'].fillna(df['bmi'].mean(),inplace = True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Final Shape of data after removing null values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    " df.isna().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.drop(['applicant_id'],axis = 1, inplace = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# seperating numerical and categorical columns\n",
    "num_cols = list(df.select_dtypes(include= ['int','float']).columns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cat_cols = list(df.select_dtypes(include = ['object']).columns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# checking length of categorical columns\n",
    "len(cat_cols)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 3.Data preparation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Univariate Analysis\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# making a dictionary having columns of df as keys and unique values in those columns as values for these keys \n",
    "unique_count_dict = {}\n",
    "for i in cat_cols:\n",
    "    a = df[i].nunique()\n",
    "    unique_count_dict[i] = a"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "unique_count_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# checking how many columns are left having uniques values \n",
    "len(unique_count_dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# checking length of numerical columns\n",
    "len(num_cols)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(num_cols)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df[num_cols].describe().T"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# checking count of all the categories in the categorical columns\n",
    "for j in unique_count_dict :\n",
    "     plt.figure(figsize=(25,5))\n",
    "     sns.countplot (x=j, data = df)\n",
    "     plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# plotting histograms for all the numerical columns  \n",
    "for col in num_cols:\n",
    "    sns.histplot(x=df[col])\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for column in df.columns:\n",
    "    if df[column].dtype == 'object':\n",
    "        print(column.upper(),': ',df[column].nunique())\n",
    "        print(df[column].value_counts().sort_values())\n",
    "        print('\\n')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Bivariate Analysis & Multivariate Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# plotting the heatmap to check for multicollinearity\n",
    "plt.figure(figsize=(50, 50))\n",
    "sns.heatmap(df[num_cols].corr(), annot=True, fmt='.2f', cmap=\"YlOrBr\", annot_kws={\"size\": 40})\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_cor = pd.DataFrame(df[num_cols].corr())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# checking for corelation\n",
    "df_cor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# resetting the index for the above DataFrame\n",
    "df_cor = df_cor.reset_index()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_cor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# checking for the columns having corelation of more than 0.6 and less than -0.6 as these will be higly corelated columns \n",
    "cor_dict = {}\n",
    "\n",
    "for i in num_cols:\n",
    "    a = df_cor[((df_cor[i]>=0.6) & (df_cor[i]<1)) | ((df_cor[i] >-1 ) & df_cor[i]<=-0.6)]\n",
    "    cor_dict[i] = list(a['index'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "a"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cor_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.pairplot(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Creating dummy columns for all the categorical columns  \n",
    "df = pd.get_dummies(df, columns=cat_cols,drop_first=True,dtype = 'int')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Number of columns are changed to 44\n",
    "df.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Checking first few rows of the DataFrame after creating dummies \n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# checking for variance in the dataset before scaling so as to choose the best scaling technique\n",
    "df.cov()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Scaling- MinMax Scalar"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Applying MinMaxScaler to all the columns of the DataFrame\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "scaler = MinMaxScaler()\n",
    "df[df.columns] = scaler.fit_transform(df[df.columns])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Checking for potential outliers in the  columns by plotting boxplot\n",
    "for i in num_cols:\n",
    "    plt.figure(figsize=(10,10))\n",
    "    sns.boxplot(data = df[i], orient = 'v')\n",
    "    plt.show() "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# As SalePrice is the target variable, therefore it has to be removed from the num_cols \n",
    "num_cols.remove('insurance_cost')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Removing Outliers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Creating a function based on the Inter Quantile Range to remove the outliers outside this range\n",
    "def remove_outlier(col):\n",
    "    sorted(col)\n",
    "    Q1,Q3=np.percentile(col,[25,75])\n",
    "    IQR=Q3-Q1\n",
    "    lower_range= Q1-(1.5 * IQR)\n",
    "    upper_range= Q3+(1.5 * IQR)\n",
    "    return lower_range, upper_range"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Converting all the values greater than Inter Quantile Range to the Upper Limit and values lower than IQR to Lower Limit \n",
    "for column in num_cols:\n",
    "    lr,ur=remove_outlier(df[column])\n",
    "    df[column]=np.where(df[column]>ur,ur,df[column])\n",
    "    df[column]=np.where(df[column]<lr,lr,df[column])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Creating boxplots to check the numerical columns after updating the columns\n",
    "for i in num_cols:\n",
    "    plt.figure(figsize=(10,10))\n",
    "    sns.boxplot(data = df[i], orient = 'v')\n",
    "    plt.show() "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 4.Model building and evaluation "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Splitting the data into training and testing set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Creating a DataFrame X containing all the independent variables \n",
    "X = df.drop('insurance_cost', axis=1)\n",
    "\n",
    "# Creating a DataFrame y having the target variable\n",
    "y = df[['insurance_cost']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Splitting the data into training and testing data in the ratio of 70:30 respectively \n",
    "from sklearn.model_selection import train_test_split\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, \n",
    "                                                    train_size=0.7,\n",
    "                                                    test_size = 0.3, random_state=100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Importing useful libraries for application of various models \n",
    "from sklearn import linear_model, metrics\n",
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn.linear_model import Ridge\n",
    "from sklearn.linear_model import Lasso\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.metrics import mean_squared_error, r2_score"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Linear Regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# invoking the LinearRegression function and finding the bestfit model on training data\n",
    "\n",
    "regression_model = LinearRegression()\n",
    "regression_model.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Let us explore the coefficients for each of the independent attributes\n",
    "\n",
    "for idx, col_name in enumerate(X_train.columns):\n",
    "    print(\"The coefficient for {} is {}\".format(col_name, regression_model.coef_[0][idx]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Let us check the intercept for the model\n",
    "\n",
    "intercept = regression_model.intercept_[0]\n",
    "\n",
    "print(\"The intercept for our model is {}\".format(intercept))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import statsmodels.api as sm\n",
    "\n",
    "# Add a constant\n",
    "X_train_lm = sm.add_constant(X_train)\n",
    "# Create a first fitted model\n",
    "lr = sm.OLS(y_train, X_train_lm).fit()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plotting the summary table of the first fitted model \n",
    "print(lr.summary())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check for the VIF values of the feature variables. \n",
    "from statsmodels.stats.outliers_influence import variance_inflation_factor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Creating a DataFrame displaying VIF of all the features \n",
    "vif = pd.DataFrame()\n",
    "vif['Features'] = X_train_lm.columns\n",
    "vif['VIF'] = [variance_inflation_factor(X_train_lm.values, i) for i in range(X_train_lm.shape[1])]\n",
    "vif['VIF'] = round(vif['VIF'], 2)\n",
    "vif = vif.sort_values(by = \"VIF\", ascending = False)\n",
    "vif[0:50]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Lets calculate some metrics such as R2 score, RSS and RMSE\n",
    "y_pred_train = regression_model.predict(X_train)\n",
    "y_pred_test = regression_model.predict(X_test)\n",
    "\n",
    "metric1 = []\n",
    "r2_train_lr = r2_score(y_train, y_pred_train)       #Calculating R2 score for training data\n",
    "print('R2 Score of Training data: ',round(r2_train_lr,3))\n",
    "metric1.append(r2_train_lr)\n",
    "\n",
    "r2_test_lr = r2_score(y_test, y_pred_test)          #Calculating R2 score for testing data\n",
    "print('R2 Score of Testing data: ',round(r2_test_lr,3))\n",
    "metric1.append(r2_test_lr)\n",
    "\n",
    "rss1_lr = np.sum(np.square(y_train - y_pred_train))  #Calculating Residual sum of squares of y_train and y_pred_train\n",
    "print('RSS1: ',round(rss1_lr[0],3))\n",
    "metric1.append(rss1_lr[0])\n",
    "\n",
    "rss2_lr = np.sum(np.square(y_test - y_pred_test))    #Calculating Residual sum of squares of y_test and y_pred_test\n",
    "print('RSS2: ',round(rss2_lr[0],3))\n",
    "metric1.append(rss2_lr[0])\n",
    "\n",
    "mse_train_lr = mean_squared_error(y_train, y_pred_train)  #Calculating Mean Squared Error of y_train and y_pred_train\n",
    "print('RMSE1: ',round(mse_train_lr**0.5,3))  #Calculating Root Mean Squared Error of y_train and y_pred_train by taking root of MSE\n",
    "metric1.append(mse_train_lr**0.5)\n",
    "\n",
    "mse_test_lr = mean_squared_error(y_test, y_pred_test)    #Calculating Mean Squared Error of y_test and y_pred_test\n",
    "print('RMSE2: ',round(mse_test_lr**0.5,3)) #Calculating Root Mean Squared Error of y_test and y_pred_test by taking root of MSE\n",
    "metric1.append(mse_test_lr**0.5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Inferences from Linear Regression:\n",
    "### 1. R2 Score of Training data:  0.945\n",
    "### 2. R2 Score of Testing data:  0.945\n",
    "### 3. RSS1:  46.154\n",
    "### 4. RSS2:  19.638\n",
    "### 5. RMSE1:  0.051\n",
    "### 6. RMSE2:  0.051\n",
    "### Now we will apply Ridge Regularisation to check we can get improved results."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Ridge Regularisation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# list of alphas to tune - if value too high it will lead to underfitting, if it is too low, \n",
    "# it will not handle the overfitting\n",
    "params = {'alpha': [0.0001, 0.001, 0.01, 0.05, 0.1, \n",
    " 0.2, 0.3, 0.4, 0.5, 0.6, 0.7, 0.8, 0.9, 1.0, 2.0, 3.0, \n",
    " 4.0, 5.0, 6.0, 7.0, 8.0, 9.0, 10.0, 20, 50, 100, 500, 1000 ]}\n",
    "\n",
    "ridge = Ridge()\n",
    "\n",
    "# cross validation\n",
    "folds = 5\n",
    "model_cv = GridSearchCV(estimator = ridge, \n",
    "                        param_grid = params, \n",
    "                        scoring= 'neg_mean_absolute_error',  \n",
    "                        cv = folds, \n",
    "                        return_train_score=True,\n",
    "                        verbose = 1)           \n",
    "model_cv.fit(X_train, y_train) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Printing the best hyperparameter alpha\n",
    "print(model_cv.best_params_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Fitting Ridge model for alpha = 5 and printing coefficients which have been penalised\n",
    "alpha = 5\n",
    "ridge = Ridge(alpha=alpha)\n",
    "\n",
    "ridge.fit(X_train, y_train)\n",
    "print(ridge.coef_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Lets calculate some metrics such as R2 score, RSS and RMSE\n",
    "y_pred_train = ridge.predict(X_train)\n",
    "y_pred_test = ridge.predict(X_test)\n",
    "\n",
    "metric2 = []\n",
    "r2_train_lr = r2_score(y_train, y_pred_train)       #Calculating R2 score for training data\n",
    "print('R2 Score of Training data: ',round(r2_train_lr,3))\n",
    "metric2.append(r2_train_lr)\n",
    "\n",
    "r2_test_lr = r2_score(y_test, y_pred_test)          #Calculating R2 score for testing data\n",
    "print('R2 Score of Testing data: ',round(r2_test_lr,3))\n",
    "metric2.append(r2_test_lr)\n",
    "\n",
    "rss1_lr = np.sum(np.square(y_train - y_pred_train))  #Calculating Residual sum of squares of y_train and y_pred_train\n",
    "print('RSS1: ',round(rss1_lr[0],3))\n",
    "metric2.append(rss1_lr[0])\n",
    "\n",
    "rss2_lr = np.sum(np.square(y_test - y_pred_test))    #Calculating Residual sum of squares of y_test and y_pred_test\n",
    "print('RSS2: ',round(rss2_lr[0],3))\n",
    "metric2.append(rss2_lr[0])\n",
    "\n",
    "mse_train_lr = mean_squared_error(y_train, y_pred_train)  #Calculating Mean Squared Error of y_train and y_pred_train\n",
    "print('RMSE1: ',round(mse_train_lr**0.5,3))  #Calculating Root Mean Squared Error of y_train and y_pred_train by taking root of MSE\n",
    "metric2.append(mse_train_lr**0.5)\n",
    "\n",
    "mse_test_lr = mean_squared_error(y_test, y_pred_test)    #Calculating Mean Squared Error of y_test and y_pred_test\n",
    "print('RMSE2: ',round(mse_test_lr**0.5,3)) #Calculating Root Mean Squared Error of y_test and y_pred_test by taking root of MSE\n",
    "metric2.append(mse_test_lr**0.5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Inferences from Ridge Regression:\n",
    "### 1. R2 Score of Training data:  0.945\n",
    "### 2. R2 Score of Testing data:  0.945\n",
    "### 3. RSS1:  46.192\n",
    "### 4. RSS2:  19.638"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Performance of Linear Regression Vs Ridge Regularization\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Creating DataFrame containing the performance scores of both the Ridge Regression and the Lasso Regression\n",
    "Performance_metric = {'Metric': ['R2 Score (Train)','R2 Score (Test)','RSS (Train)','RSS (Test)',\n",
    "                       'RMSE (Train)','RMSE (Test)'], \n",
    "        'Linear Regression': metric1,\n",
    "        'Ridge Regression': metric2\n",
    "        }\n",
    "Performance_metric = pd.DataFrame(Performance_metric)\n",
    "Performance_metric"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'df' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Input \u001b[1;32mIn [1]\u001b[0m, in \u001b[0;36m<cell line: 1>\u001b[1;34m()\u001b[0m\n\u001b[1;32m----> 1\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m i \u001b[38;5;129;01min\u001b[39;00m \u001b[43mdf\u001b[49m[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mFeatures\u001b[39m\u001b[38;5;124m'\u001b[39m]:     \u001b[38;5;66;03m#Printing all the coefficients of Lasso Regression model along with the feature names\u001b[39;00m\n\u001b[0;32m      2\u001b[0m     \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m(\u001b[39m\u001b[38;5;132;01m{}\u001b[39;00m\u001b[38;5;124m) * \u001b[39m\u001b[38;5;132;01m{}\u001b[39;00m\u001b[38;5;124m \u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;241m.\u001b[39mformat(\u001b[38;5;28mround\u001b[39m(final_df[final_df[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mFeatures\u001b[39m\u001b[38;5;124m\"\u001b[39m]\u001b[38;5;241m==\u001b[39mi]\u001b[38;5;241m.\u001b[39miloc[:,\u001b[38;5;241m2\u001b[39m]\u001b[38;5;241m.\u001b[39mvalues[\u001b[38;5;241m0\u001b[39m],\u001b[38;5;241m2\u001b[39m),i),end\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m+\u001b[39m\u001b[38;5;124m'\u001b[39m)\n",
      "\u001b[1;31mNameError\u001b[0m: name 'df' is not defined"
     ]
    }
   ],
   "source": [
    "for i in df['Features']:     #Printing all the coefficients of Lasso Regression model along with the feature names\n",
    "    print('({}) * {} '.format(round(final_df[final_df[\"Features\"]==i].iloc[:,2].values[0],2),i),end='+')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Final Conclusion:\n",
    "## Both Linear and Ridge Regression are performing well on our dataset. Therefore, Linear regression is performing perfectly."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# End"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
